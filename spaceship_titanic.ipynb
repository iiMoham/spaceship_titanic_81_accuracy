{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "WncBTwmshxKE"
      },
      "source": [
        "# Spaceship Titanic - Enhanced Solution (Target: >0.80)\n",
        "\n",
        "This notebook implements advanced techniques to improve upon your 0.80 score:\n",
        "\n",
        "1. **Advanced Feature Engineering**: Extract hidden patterns from PassengerId, Cabin, Names\n",
        "2. **Smart Imputation**: KNN imputer for missing values\n",
        "3. **Interaction Features**: Capture relationships between features\n",
        "4. **Ensemble Methods**: Combine multiple models for robust predictions\n",
        "5. **Cross-validation**: Ensure model generalizes well\n",
        "\n",
        "Let's start by importing necessary libraries and loading the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost"
      ],
      "metadata": {
        "id": "6NZskg5viDaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzr_xedghxKG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold, GridSearchCV, train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, ExtraTreesClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.impute import KNNImputer, SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from catboost import CatBoostClassifier\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "np.random.seed(42)\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a directory for the Kaggle API token\n",
        "!mkdir -p ~/.kaggle\n",
        "\n",
        "# Move the uploaded token to the correct directory\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# Set the necessary permissions for the token\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "c7ZgHotbihjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c spaceship-titanic"
      ],
      "metadata": {
        "id": "KvkySNO5i4A9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/spaceship-titanic.zip"
      ],
      "metadata": {
        "id": "k-mGhvjPi8ed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "wdUyWpYkhxKH"
      },
      "source": [
        "## 1. Load and Explore Data\n",
        "\n",
        "Let's load the data and understand its structure. Pay special attention to the PassengerId format (gggg_pp) and Cabin format (deck/num/side).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZ17li7ohxKH"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv('/content/test.csv')\n",
        "\n",
        "print(f\"Training data shape: {train_df.shape}\")\n",
        "print(f\"Test data shape: {test_df.shape}\")\n",
        "print(\"\\nFirst few rows of training data:\")\n",
        "train_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "brepDPWKhxKH"
      },
      "outputs": [],
      "source": [
        "# Check missing values\n",
        "print(\"Missing values in training data:\")\n",
        "print(train_df.isnull().sum())\n",
        "print(\"\\nData types:\")\n",
        "print(train_df.dtypes)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "e2HKx7OVhxKI"
      },
      "source": [
        "## 2. Key Insights for Feature Engineering\n",
        "\n",
        "### Important patterns to exploit:\n",
        "1. **CryoSleep**: Passengers in cryosleep are confined to cabins and shouldn't have any spending\n",
        "2. **Group patterns**: Families/groups tend to have similar outcomes\n",
        "3. **Cabin structure**: deck/num/side - different decks might have different survival rates\n",
        "4. **Age groups**: Children and elderly might have different patterns\n",
        "5. **VIP status**: Might correlate with spending and cabin location\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEwKtFCshxKI"
      },
      "outputs": [],
      "source": [
        "# Feature Engineering Functions\n",
        "\n",
        "def extract_group_features(df):\n",
        "    \"\"\"Extract group-related features from PassengerId\"\"\"\n",
        "    df = df.copy()\n",
        "    df['Group'] = df['PassengerId'].str.split('_').str[0].astype(int)\n",
        "    df['GroupPosition'] = df['PassengerId'].str.split('_').str[1].astype(int)\n",
        "\n",
        "    # Group size\n",
        "    group_size = df.groupby('Group').size().reset_index(name='GroupSize')\n",
        "    df = df.merge(group_size, on='Group', how='left')\n",
        "\n",
        "    # Solo traveler indicator\n",
        "    df['IsSolo'] = (df['GroupSize'] == 1).astype(int)\n",
        "\n",
        "    # Family indicator (assuming groups of 2-4 are likely families)\n",
        "    df['IsFamily'] = ((df['GroupSize'] >= 2) & (df['GroupSize'] <= 4)).astype(int)\n",
        "\n",
        "    # Large group indicator\n",
        "    df['IsLargeGroup'] = (df['GroupSize'] > 4).astype(int)\n",
        "\n",
        "    # Position in group (first, middle, last)\n",
        "    df['IsFirstInGroup'] = (df['GroupPosition'] == 1).astype(int)\n",
        "    df['IsLastInGroup'] = (df['GroupPosition'] == df['GroupSize']).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def extract_cabin_features(df):\n",
        "    \"\"\"Extract features from Cabin column\"\"\"\n",
        "    df = df.copy()\n",
        "    # Split cabin into deck, num, side\n",
        "    cabin_split = df['Cabin'].str.split('/', expand=True)\n",
        "    df['Deck'] = cabin_split[0]\n",
        "    df['CabinNum'] = pd.to_numeric(cabin_split[1], errors='coerce')\n",
        "    df['Side'] = cabin_split[2]\n",
        "\n",
        "    # Create deck level (A=1, B=2, etc.)\n",
        "    deck_map = {'A': 1, 'B': 2, 'C': 3, 'D': 4, 'E': 5, 'F': 6, 'G': 7, 'T': 8}\n",
        "    df['DeckLevel'] = df['Deck'].map(deck_map)\n",
        "\n",
        "    # Port/Starboard encoding\n",
        "    df['IsPort'] = (df['Side'] == 'P').astype(int)\n",
        "\n",
        "    # Cabin region (front, middle, back of ship)\n",
        "    df['CabinRegion'] = pd.cut(df['CabinNum'], bins=[0, 300, 600, 2000],\n",
        "                               labels=['Front', 'Middle', 'Back'], include_lowest=True)\n",
        "\n",
        "    # Special decks\n",
        "    df['IsTopDeck'] = df['Deck'].isin(['A', 'B', 'T']).astype(int)\n",
        "    df['IsBottomDeck'] = df['Deck'].isin(['F', 'G']).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def extract_name_features(df):\n",
        "    \"\"\"Extract features from Name column\"\"\"\n",
        "    df = df.copy()\n",
        "    # Extract last name\n",
        "    df['LastName'] = df['Name'].str.split(' ').str[0]\n",
        "\n",
        "    # Count of people with same last name (potential family members)\n",
        "    lastname_count = df.groupby('LastName').size().reset_index(name='LastNameCount')\n",
        "    df = df.merge(lastname_count, on='LastName', how='left')\n",
        "\n",
        "    # Has family member indicator\n",
        "    df['HasFamilyMember'] = (df['LastNameCount'] > 1).astype(int)\n",
        "\n",
        "    # Name length (might correlate with social status)\n",
        "    df['NameLength'] = df['Name'].fillna('').str.len()\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_spending_features(df):\n",
        "    \"\"\"Create features from spending columns\"\"\"\n",
        "    df = df.copy()\n",
        "    spending_cols = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
        "\n",
        "    # Total spending\n",
        "    df['TotalSpending'] = df[spending_cols].sum(axis=1)\n",
        "\n",
        "    # Number of amenities used\n",
        "    df['AmenitiesUsed'] = (df[spending_cols] > 0).sum(axis=1)\n",
        "\n",
        "    # Spending categories\n",
        "    df['IsHighSpender'] = (df['TotalSpending'] > df['TotalSpending'].quantile(0.75)).astype(int)\n",
        "    df['IsZeroSpender'] = (df['TotalSpending'] == 0).astype(int)\n",
        "\n",
        "    # Spending ratios\n",
        "    for col in spending_cols:\n",
        "        df[f'{col}_Ratio'] = df[col] / (df['TotalSpending'] + 1)\n",
        "\n",
        "    # Luxury spending (Spa + VRDeck)\n",
        "    df['LuxurySpending'] = df['Spa'] + df['VRDeck']\n",
        "    df['LuxuryRatio'] = df['LuxurySpending'] / (df['TotalSpending'] + 1)\n",
        "\n",
        "    # Basic spending (RoomService + FoodCourt)\n",
        "    df['BasicSpending'] = df['RoomService'] + df['FoodCourt']\n",
        "    df['BasicRatio'] = df['BasicSpending'] / (df['TotalSpending'] + 1)\n",
        "\n",
        "    # Entertainment spending\n",
        "    df['EntertainmentSpending'] = df['ShoppingMall'] + df['VRDeck']\n",
        "\n",
        "    # Spending variance (how varied is their spending)\n",
        "    df['SpendingVariance'] = df[spending_cols].var(axis=1)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_interaction_features(df):\n",
        "    \"\"\"Create interaction features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # CryoSleep interactions (key insight: cryosleep passengers shouldn't spend)\n",
        "    df['Cryo_Age'] = df['CryoSleep'] * df['Age']\n",
        "    df['Cryo_VIP'] = df['CryoSleep'] * df['VIP']\n",
        "    df['Cryo_TotalSpending'] = df['CryoSleep'] * df['TotalSpending']\n",
        "    df['Cryo_Anomaly'] = ((df['CryoSleep'] == 1) & (df['TotalSpending'] > 0)).astype(int)\n",
        "\n",
        "    # Age interactions\n",
        "    df['Age_VIP'] = df['Age'] * df['VIP']\n",
        "    df['Age_TotalSpending'] = df['Age'] * df['TotalSpending']\n",
        "    df['Age_GroupSize'] = df['Age'] * df['GroupSize']\n",
        "\n",
        "    # Group interactions\n",
        "    df['GroupSize_TotalSpending'] = df['GroupSize'] * df['TotalSpending']\n",
        "    df['IsSolo_Age'] = df['IsSolo'] * df['Age']\n",
        "    df['IsSolo_Spending'] = df['IsSolo'] * df['TotalSpending']\n",
        "\n",
        "    # Planet-Destination interaction\n",
        "    df['SamePlanetDest'] = (df['HomePlanet'] == df['Destination']).astype(int)\n",
        "\n",
        "    # VIP interactions\n",
        "    df['VIP_DeckLevel'] = df['VIP'] * df['DeckLevel']\n",
        "    df['VIP_LuxurySpending'] = df['VIP'] * df['LuxurySpending']\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_age_features(df):\n",
        "    \"\"\"Create age-related features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Age groups\n",
        "    df['AgeGroup'] = pd.cut(df['Age'], bins=[0, 12, 18, 35, 50, 65, 100],\n",
        "                           labels=['Child', 'Teen', 'YoungAdult', 'Adult', 'Senior', 'Elder'])\n",
        "\n",
        "    # Missing age indicator\n",
        "    df['AgeMissing'] = df['Age'].isna().astype(int)\n",
        "\n",
        "    # Age statistics within group\n",
        "    age_group_stats = df.groupby('Group')['Age'].agg(['mean', 'min', 'max', 'std']).reset_index()\n",
        "    age_group_stats.columns = ['Group', 'GroupAgeMean', 'GroupAgeMin', 'GroupAgeMax', 'GroupAgeStd']\n",
        "    df = df.merge(age_group_stats, on='Group', how='left')\n",
        "\n",
        "    # Is youngest/oldest in group\n",
        "    df['IsYoungestInGroup'] = (df['Age'] == df['GroupAgeMin']).astype(int)\n",
        "    df['IsOldestInGroup'] = (df['Age'] == df['GroupAgeMax']).astype(int)\n",
        "\n",
        "    return df\n",
        "\n",
        "def create_advanced_features(df):\n",
        "    \"\"\"Create additional advanced features\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Spending patterns by group\n",
        "    group_spending = df.groupby('Group')['TotalSpending'].agg(['mean', 'sum', 'std']).reset_index()\n",
        "    group_spending.columns = ['Group', 'GroupSpendingMean', 'GroupSpendingSum', 'GroupSpendingStd']\n",
        "    df = df.merge(group_spending, on='Group', how='left')\n",
        "\n",
        "    # Individual vs group spending ratio\n",
        "    df['SpendingVsGroupMean'] = df['TotalSpending'] / (df['GroupSpendingMean'] + 1)\n",
        "\n",
        "    # Cabin mate features (people in same cabin)\n",
        "    cabin_counts = df.groupby('Cabin').size().reset_index(name='CabinMates')\n",
        "    df = df.merge(cabin_counts, on='Cabin', how='left')\n",
        "    df['HasCabinMate'] = (df['CabinMates'] > 1).astype(int)\n",
        "\n",
        "    # Anomaly detection features\n",
        "    df['VIP_NoSpending'] = ((df['VIP'] == 1) & (df['TotalSpending'] == 0)).astype(int)\n",
        "    df['NonVIP_HighSpending'] = ((df['VIP'] == 0) & (df['IsHighSpender'] == 1)).astype(int)\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "ydo6hQA0hxKI"
      },
      "source": [
        "## 3. Advanced Preprocessing Pipeline\n",
        "\n",
        "Now let's create a comprehensive preprocessing function that handles missing values intelligently and applies all our feature engineering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tiJqDjf7hxKJ"
      },
      "outputs": [],
      "source": [
        "def preprocess_data(train_df, test_df):\n",
        "    \"\"\"Main preprocessing function with advanced imputation and feature engineering\"\"\"\n",
        "    # Combine train and test for consistent preprocessing\n",
        "    train_df = train_df.copy()\n",
        "    test_df = test_df.copy()\n",
        "    train_df['is_train'] = 1\n",
        "    test_df['is_train'] = 0\n",
        "    df = pd.concat([train_df, test_df], ignore_index=True)\n",
        "\n",
        "    # Save PassengerId for later\n",
        "    passenger_ids = df['PassengerId'].copy()\n",
        "\n",
        "    # Extract all features\n",
        "    print(\"Extracting features...\")\n",
        "    df = extract_group_features(df)\n",
        "    df = extract_cabin_features(df)\n",
        "    df = extract_name_features(df)\n",
        "    df = create_spending_features(df)\n",
        "    df = create_age_features(df)\n",
        "    df = create_interaction_features(df)\n",
        "    df = create_advanced_features(df)\n",
        "\n",
        "    # Handle missing values\n",
        "    print(\"Handling missing values...\")\n",
        "\n",
        "    # Numerical features - use KNN imputer\n",
        "    numerical_features = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck',\n",
        "                         'CabinNum', 'DeckLevel', 'GroupAgeMean', 'GroupAgeMin', 'GroupAgeMax',\n",
        "                         'GroupAgeStd', 'GroupSpendingMean', 'GroupSpendingSum', 'GroupSpendingStd']\n",
        "\n",
        "    # Remove features that might not exist\n",
        "    numerical_features = [f for f in numerical_features if f in df.columns]\n",
        "\n",
        "    # Use KNN imputer for numerical features\n",
        "    imputer = KNNImputer(n_neighbors=5)\n",
        "    df[numerical_features] = imputer.fit_transform(df[numerical_features])\n",
        "\n",
        "    # Fill categorical missing values\n",
        "    categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP', 'Deck', 'Side',\n",
        "                           'AgeGroup', 'CabinRegion']\n",
        "\n",
        "    for col in categorical_features:\n",
        "        if col in df.columns:\n",
        "            # Fill with mode for most columns\n",
        "            if col in ['CryoSleep', 'VIP']:\n",
        "                df[col] = df[col].fillna(False)\n",
        "            else:\n",
        "                mode_val = df[col].mode()\n",
        "                if len(mode_val) > 0:\n",
        "                    df[col] = df[col].fillna(mode_val[0])\n",
        "                else:\n",
        "                    df[col] = df[col].fillna('Unknown')\n",
        "\n",
        "    # Encode categorical variables\n",
        "    print(\"Encoding categorical variables...\")\n",
        "    label_encoders = {}\n",
        "\n",
        "    # One-hot encode some features\n",
        "    one_hot_features = ['HomePlanet', 'Destination', 'Deck', 'CabinRegion']\n",
        "    for col in one_hot_features:\n",
        "        if col in df.columns:\n",
        "            dummies = pd.get_dummies(df[col], prefix=col)\n",
        "            df = pd.concat([df, dummies], axis=1)\n",
        "            df = df.drop(col, axis=1)\n",
        "\n",
        "    # Label encode other categorical features\n",
        "    label_encode_features = ['AgeGroup', 'Side']\n",
        "    for col in label_encode_features:\n",
        "        if col in df.columns:\n",
        "            le = LabelEncoder()\n",
        "            df[col] = le.fit_transform(df[col].astype(str))\n",
        "            label_encoders[col] = le\n",
        "\n",
        "    # Convert boolean columns to int\n",
        "    bool_cols = ['CryoSleep', 'VIP']\n",
        "    for col in bool_cols:\n",
        "        if col in df.columns:\n",
        "            df[col] = df[col].astype(int)\n",
        "\n",
        "    # Add back PassengerId\n",
        "    df['PassengerId'] = passenger_ids\n",
        "\n",
        "    # Split back to train and test\n",
        "    train_df = df[df['is_train'] == 1].drop('is_train', axis=1)\n",
        "    test_df = df[df['is_train'] == 0].drop('is_train', axis=1)\n",
        "\n",
        "    return train_df, test_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KtosTxGhxKJ"
      },
      "outputs": [],
      "source": [
        "# Apply preprocessing\n",
        "print(\"Preprocessing data...\")\n",
        "train_processed, test_processed = preprocess_data(train_df, test_df)\n",
        "\n",
        "print(f\"Processed training data shape: {train_processed.shape}\")\n",
        "print(f\"Processed test data shape: {test_processed.shape}\")\n",
        "\n",
        "# Save test PassengerIds for submission\n",
        "test_passenger_ids = test_df['PassengerId']\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def fix_nan_values(df):\n",
        "    \"\"\"Fix any NaN values that might have been introduced during feature engineering\"\"\"\n",
        "    # Replace inf values with NaN first\n",
        "    df = df.replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "    # Fill NaN values in different types of columns\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['float64', 'float32', 'int64', 'int32']:\n",
        "            # For numerical columns, fill with median\n",
        "            if df[col].isnull().any():\n",
        "                df[col] = df[col].fillna(df[col].median())\n",
        "        else:\n",
        "            # For other columns, fill with mode or 0\n",
        "            if df[col].isnull().any():\n",
        "                mode_val = df[col].mode()\n",
        "                if len(mode_val) > 0:\n",
        "                    df[col] = df[col].fillna(mode_val[0])\n",
        "                else:\n",
        "                    df[col] = df[col].fillna(0)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "m4JcaYU3k1EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the NaN fixing to our processed data\n",
        "print(\"Applying NaN fixes to processed data...\")\n",
        "train_processed = fix_nan_values(train_processed)\n",
        "test_processed = fix_nan_values(test_processed)\n",
        "\n",
        "# Verify no NaN values remain\n",
        "train_nan_count = train_processed.isnull().sum().sum()\n",
        "test_nan_count = test_processed.isnull().sum().sum()\n",
        "print(f\"NaN values in train after fixing: {train_nan_count}\")\n",
        "print(f\"NaN values in test after fixing: {test_nan_count}\")\n"
      ],
      "metadata": {
        "id": "mGDnFvjzk4Rg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "aKATLZrghxKJ"
      },
      "source": [
        "## 4. Model Training with Advanced Ensemble\n",
        "\n",
        "We'll use a sophisticated ensemble approach combining:\n",
        "- Random Forest (good for capturing non-linear patterns)\n",
        "- XGBoost (powerful gradient boosting)\n",
        "- LightGBM (fast and efficient)\n",
        "- CatBoost (handles categorical features well)\n",
        "- Logistic Regression (for linear patterns)\n",
        "\n",
        "We'll use soft voting to combine predictions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xqCpcrihxKK"
      },
      "outputs": [],
      "source": [
        "# Prepare features for modeling\n",
        "features_to_drop = ['PassengerId', 'Name', 'LastName', 'Cabin', 'Transported', 'Group']\n",
        "X = train_processed.drop(features_to_drop, axis=1, errors='ignore')\n",
        "y = train_processed['Transported'].astype(int)\n",
        "X_test = test_processed.drop(features_to_drop, axis=1, errors='ignore')\n",
        "\n",
        "# Ensure same columns in train and test\n",
        "common_cols = list(set(X.columns) & set(X_test.columns))\n",
        "X = X[common_cols]\n",
        "X_test = X_test[common_cols]\n",
        "\n",
        "print(f\"Number of features: {len(common_cols)}\")\n",
        "print(f\"Training shape: {X.shape}\")\n",
        "print(f\"Test shape: {X_test.shape}\")\n",
        "\n",
        "# Scale features using RobustScaler (handles outliers better)\n",
        "scaler = RobustScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_test_scaled = scaler.transform(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xLKU81_hxKK"
      },
      "outputs": [],
      "source": [
        "# Create base models with optimized hyperparameters\n",
        "print(\"Creating ensemble models...\")\n",
        "\n",
        "# Random Forest\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=12,\n",
        "    min_samples_split=4,\n",
        "    min_samples_leaf=2,\n",
        "    max_features='sqrt',\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Extra Trees (similar to RF but more random)\n",
        "et = ExtraTreesClassifier(\n",
        "    n_estimators=400,\n",
        "    max_depth=12,\n",
        "    min_samples_split=4,\n",
        "    min_samples_leaf=2,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Gradient Boosting\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=300,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=4,\n",
        "    min_samples_split=4,\n",
        "    subsample=0.8,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# XGBoost\n",
        "xgb_model = xgb.XGBClassifier(\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=5,\n",
        "    min_child_weight=2,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    gamma=0.1,\n",
        "    random_state=42,\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "# LightGBM\n",
        "lgb_model = lgb.LGBMClassifier(\n",
        "    n_estimators=400,\n",
        "    learning_rate=0.03,\n",
        "    max_depth=5,\n",
        "    num_leaves=25,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    random_state=42,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# CatBoost\n",
        "cb_model = CatBoostClassifier(\n",
        "    iterations=400,\n",
        "    learning_rate=0.03,\n",
        "    depth=6,\n",
        "    l2_leaf_reg=3,\n",
        "    bootstrap_type='Bernoulli',\n",
        "    subsample=0.8,\n",
        "    random_seed=42,\n",
        "    verbose=False\n",
        ")\n",
        "\n",
        "# Logistic Regression\n",
        "lr = LogisticRegression(\n",
        "    C=0.1,\n",
        "    max_iter=2000,\n",
        "    solver='liblinear',\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# SVM\n",
        "svm = SVC(\n",
        "    C=1.0,\n",
        "    kernel='rbf',\n",
        "    gamma='scale',\n",
        "    probability=True,\n",
        "    random_state=42\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3wU9jJdKhxKK"
      },
      "outputs": [],
      "source": [
        "# Cross-validation to evaluate models\n",
        "print(\"Performing cross-validation...\")\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "models = {\n",
        "    'Random Forest': rf,\n",
        "    'Extra Trees': et,\n",
        "    'Gradient Boosting': gb,\n",
        "    'XGBoost': xgb_model,\n",
        "    'LightGBM': lgb_model,\n",
        "    'CatBoost': cb_model,\n",
        "    'Logistic Regression': lr,\n",
        "    'SVM': svm\n",
        "}\n",
        "\n",
        "cv_scores = {}\n",
        "for name, model in models.items():\n",
        "    scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "    cv_scores[name] = scores\n",
        "    print(f\"{name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
        "\n",
        "# Create voting classifier with best models\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('rf', rf),\n",
        "        ('et', et),\n",
        "        ('gb', gb),\n",
        "        ('xgb', xgb_model),\n",
        "        ('lgb', lgb_model),\n",
        "        ('cb', cb_model)\n",
        "    ],\n",
        "    voting='soft',\n",
        "    weights=[1.2, 1.0, 1.1, 1.3, 1.3, 1.2]  # Weight better models more\n",
        ")\n",
        "\n",
        "# Evaluate ensemble\n",
        "ensemble_scores = cross_val_score(voting_clf, X_scaled, y, cv=cv, scoring='accuracy', n_jobs=-1)\n",
        "print(f\"\\nEnsemble: {ensemble_scores.mean():.4f} (+/- {ensemble_scores.std() * 2:.4f})\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        },
        "id": "5Xn-r4emhxKK"
      },
      "source": [
        "## 5. Train Final Model and Generate Predictions\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m-IBo004hxKK"
      },
      "outputs": [],
      "source": [
        "# Train on full dataset\n",
        "print(\"Training final ensemble on full dataset...\")\n",
        "voting_clf.fit(X_scaled, y)\n",
        "\n",
        "# Make predictions\n",
        "predictions = voting_clf.predict(X_test_scaled)\n",
        "\n",
        "# Create submission\n",
        "submission = pd.DataFrame({\n",
        "    'PassengerId': test_passenger_ids,\n",
        "    'Transported': predictions.astype(bool)\n",
        "})\n",
        "\n",
        "# Save submission\n",
        "submission.to_csv('submission_enhanced.csv', index=False)\n",
        "print(f\"Submission saved! Shape: {submission.shape}\")\n",
        "print(\"\\nFirst few predictions:\")\n",
        "print(submission.head(10))\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}